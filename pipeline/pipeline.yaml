apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: ml-pipeline-
  annotations:
    pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
    pipelines.kubeflow.org/pipeline_compilation_time: '2023-07-05T15:55:37.093205'
    pipelines.kubeflow.org/pipeline_spec: '{"inputs": [{"name": "url", "type": "String"},
      {"default": "", "name": "pipeline-output-directory"}, {"default": "pipeline/ml-pipeline",
      "name": "pipeline-name"}], "name": "ml-pipeline"}'
    pipelines.kubeflow.org/v2_pipeline: "true"
  labels:
    pipelines.kubeflow.org/v2_pipeline: "true"
    pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
spec:
  entrypoint: ml-pipeline
  templates:
  - name: download-data
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'pandas' 'openpyxl' 'appengine-python-standard' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'pandas'
        'openpyxl' 'appengine-python-standard' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def download_data(url:str, output_csv:Output[Dataset]) -> None:
            import pandas as pd

            df = pd.read_excel(url)
            df = df.sample(frac=1).reset_index(drop=True)
            df.to_csv(output_csv.path, index=False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - download_data
      - --url-output-path
      - '{{$.inputs.parameters[''url'']}}'
      - --output-csv-output-path
      - '{{$.outputs.artifacts[''output_csv''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, download-data, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, 'url={{inputs.parameters.url}}',
        --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {"url": {"type": "STRING"}},
          "inputArtifacts": {}, "outputParameters": {}, "outputArtifacts": {"output_csv":
          {"schemaTitle": "system.Dataset", "instanceSchema": "", "metadataPath":
          "/tmp/outputs/output_csv/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      - {name: url}
    outputs:
      artifacts:
      - {name: download-data-output_csv, path: /tmp/outputs/output_csv/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
        pipelines.kubeflow.org/arguments.parameters: '{"url": "{{inputs.parameters.url}}"}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: eval-model
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'tensorflow' 'pandas' 'appengine-python-standard' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'tensorflow'
        'pandas' 'appengine-python-standard' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def eval_model(
            input_model: Input[Model], input_history: Input[Artifact],
            input_test_x: Input[Dataset], input_test_y: Input[Artifact],
            MLPipeline_Metrics: Output[Metrics]
        ):
            import pandas as pd
            import tensorflow as tf
            import pickle

            model = tf.keras.models.load_model(input_model.path)
            norm_test_X = pd.read_csv(input_test_x.path)
            with open(input_test_y.path, "rb") as file:
                test_Y = pickle.load(file)

            loss, Y1_loss, Y2_loss, Y1_rmse, Y2_rmse = model.evaluate(x=norm_test_X, y=test_Y)
            print("Loss = {}, Y1_loss = {}, Y1_mse = {}, Y2_loss = {}, Y2_mse = {}".format(loss, Y1_loss, Y1_rmse, Y2_loss, Y2_rmse))

            MLPipeline_Metrics.log_metric("loss", loss)
            MLPipeline_Metrics.log_metric("Y1_loss", Y2_loss)
            MLPipeline_Metrics.log_metric("Y2_loss", Y2_loss)
            MLPipeline_Metrics.log_metric("Y1_rmse", Y2_loss)
            MLPipeline_Metrics.log_metric("Y2_rmse", Y2_loss)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - eval_model
      - --input-model-output-path
      - '{{$.inputs.artifacts[''input_model''].path}}'
      - --input-history-output-path
      - '{{$.inputs.artifacts[''input_history''].path}}'
      - --input-test-x-output-path
      - '{{$.inputs.artifacts[''input_test_x''].path}}'
      - --input-test-y-output-path
      - '{{$.inputs.artifacts[''input_test_y''].path}}'
      - --MLPipeline-Metrics-output-path
      - '{{$.outputs.artifacts[''MLPipeline_Metrics''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, eval-model, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"input_history": {"metadataPath": "/tmp/inputs/input_history/data", "schemaTitle":
          "system.Artifact", "instanceSchema": ""}, "input_model": {"metadataPath":
          "/tmp/inputs/input_model/data", "schemaTitle": "system.Model", "instanceSchema":
          ""}, "input_test_x": {"metadataPath": "/tmp/inputs/input_test_x/data", "schemaTitle":
          "system.Dataset", "instanceSchema": ""}, "input_test_y": {"metadataPath":
          "/tmp/inputs/input_test_y/data", "schemaTitle": "system.Artifact", "instanceSchema":
          ""}}, "outputParameters": {}, "outputArtifacts": {"MLPipeline_Metrics":
          {"schemaTitle": "system.Metrics", "instanceSchema": "", "metadataPath":
          "/tmp/outputs/MLPipeline_Metrics/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      artifacts:
      - {name: train-model-output_history, path: /tmp/inputs/input_history/data}
      - {name: train-model-output_model, path: /tmp/inputs/input_model/data}
      - {name: preprocess-data-output_test_x, path: /tmp/inputs/input_test_x/data}
      - {name: preprocess-data-output_test_y, path: /tmp/inputs/input_test_y/data}
    outputs:
      artifacts:
      - {name: eval-model-MLPipeline_Metrics, path: /tmp/outputs/MLPipeline_Metrics/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: ml-pipeline
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      - {name: url}
    dag:
      tasks:
      - name: download-data
        template: download-data
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          - {name: url, value: '{{inputs.parameters.url}}'}
      - name: eval-model
        template: eval-model
        dependencies: [preprocess-data, train-model]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          artifacts:
          - {name: preprocess-data-output_test_x, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-output_test_x}}'}
          - {name: preprocess-data-output_test_y, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-output_test_y}}'}
          - {name: train-model-output_history, from: '{{tasks.train-model.outputs.artifacts.train-model-output_history}}'}
          - {name: train-model-output_model, from: '{{tasks.train-model.outputs.artifacts.train-model-output_model}}'}
      - name: preprocess-data
        template: preprocess-data
        dependencies: [split-data]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          artifacts:
          - {name: split-data-test_csv, from: '{{tasks.split-data.outputs.artifacts.split-data-test_csv}}'}
          - {name: split-data-train_csv, from: '{{tasks.split-data.outputs.artifacts.split-data-train_csv}}'}
      - name: split-data
        template: split-data
        dependencies: [download-data]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          artifacts:
          - {name: download-data-output_csv, from: '{{tasks.download-data.outputs.artifacts.download-data-output_csv}}'}
      - name: train-model
        template: train-model
        dependencies: [preprocess-data]
        arguments:
          parameters:
          - {name: pipeline-name, value: '{{inputs.parameters.pipeline-name}}'}
          - {name: pipeline-output-directory, value: '{{inputs.parameters.pipeline-output-directory}}'}
          artifacts:
          - {name: preprocess-data-output_train_x, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-output_train_x}}'}
          - {name: preprocess-data-output_train_y, from: '{{tasks.preprocess-data.outputs.artifacts.preprocess-data-output_train_y}}'}
  - name: preprocess-data
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'pandas' 'numpy' 'appengine-python-standard' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'pandas'
        'numpy' 'appengine-python-standard' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def preprocess_data(
            input_train_csv: Input[Dataset], input_test_csv: Input[Dataset],
            output_train_x: Output[Dataset], output_test_x: Output[Dataset],
            output_train_y: Output[Artifact], output_test_y: Output[Artifact]
        ):
            import pandas as pd
            import numpy as np
            import pickle

            def format_output(data):
                y1 = data.pop('Y1')
                y1 = np.array(y1)
                y2 = data.pop('Y2')
                y2 = np.array(y2)
                return y1, y2

            def norm(x, train_stats):
                return (x - train_stats['mean']) / train_stats['std']

            train = pd.read_csv(input_train_csv.path)
            test = pd.read_csv(input_test_csv.path)

            train_stats = train.describe()
            train_stats.pop('Y1')
            train_stats.pop('Y2')
            train_stats = train_stats.transpose()

            train_Y = format_output(train)
            with open(output_train_y.path, "wb") as file:
                pickle.dump(train_Y, file)

            test_Y = format_output(test)
            with open(output_test_y.path, "wb") as file:
                pickle.dump(test_Y, file)

            norm_train_x = norm(train, train_stats)
            norm_test_x = norm(test, train_stats)

            norm_train_x.to_csv(output_train_x.path, index=False)
            norm_test_x.to_csv(output_test_x.path, index=False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - preprocess_data
      - --input-train-csv-output-path
      - '{{$.inputs.artifacts[''input_train_csv''].path}}'
      - --input-test-csv-output-path
      - '{{$.inputs.artifacts[''input_test_csv''].path}}'
      - --output-train-x-output-path
      - '{{$.outputs.artifacts[''output_train_x''].path}}'
      - --output-test-x-output-path
      - '{{$.outputs.artifacts[''output_test_x''].path}}'
      - --output-train-y-output-path
      - '{{$.outputs.artifacts[''output_train_y''].path}}'
      - --output-test-y-output-path
      - '{{$.outputs.artifacts[''output_test_y''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, preprocess-data, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"input_test_csv": {"metadataPath": "/tmp/inputs/input_test_csv/data", "schemaTitle":
          "system.Dataset", "instanceSchema": ""}, "input_train_csv": {"metadataPath":
          "/tmp/inputs/input_train_csv/data", "schemaTitle": "system.Dataset", "instanceSchema":
          ""}}, "outputParameters": {}, "outputArtifacts": {"output_test_x": {"schemaTitle":
          "system.Dataset", "instanceSchema": "", "metadataPath": "/tmp/outputs/output_test_x/data"},
          "output_test_y": {"schemaTitle": "system.Artifact", "instanceSchema": "",
          "metadataPath": "/tmp/outputs/output_test_y/data"}, "output_train_x": {"schemaTitle":
          "system.Dataset", "instanceSchema": "", "metadataPath": "/tmp/outputs/output_train_x/data"},
          "output_train_y": {"schemaTitle": "system.Artifact", "instanceSchema": "",
          "metadataPath": "/tmp/outputs/output_train_y/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      artifacts:
      - {name: split-data-test_csv, path: /tmp/inputs/input_test_csv/data}
      - {name: split-data-train_csv, path: /tmp/inputs/input_train_csv/data}
    outputs:
      artifacts:
      - {name: preprocess-data-output_test_x, path: /tmp/outputs/output_test_x/data}
      - {name: preprocess-data-output_test_y, path: /tmp/outputs/output_test_y/data}
      - {name: preprocess-data-output_train_x, path: /tmp/outputs/output_train_x/data}
      - {name: preprocess-data-output_train_y, path: /tmp/outputs/output_train_y/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: split-data
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'pandas' 'scikit-learn' 'appengine-python-standard' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'pandas'
        'scikit-learn' 'appengine-python-standard' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def split_data(input_csv: Input[Dataset], train_csv: Output[Dataset], test_csv: Output[Dataset]):
            import pandas as pd
            from sklearn.model_selection import train_test_split

            df = pd.read_csv(input_csv.path)
            train, test = train_test_split(df, test_size=0.2)
            train.to_csv(train_csv.path, index=False)
            test.to_csv(test_csv.path, index=False)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - split_data
      - --input-csv-output-path
      - '{{$.inputs.artifacts[''input_csv''].path}}'
      - --train-csv-output-path
      - '{{$.outputs.artifacts[''train_csv''].path}}'
      - --test-csv-output-path
      - '{{$.outputs.artifacts[''test_csv''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, split-data, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"input_csv": {"metadataPath": "/tmp/inputs/input_csv/data", "schemaTitle":
          "system.Dataset", "instanceSchema": ""}}, "outputParameters": {}, "outputArtifacts":
          {"test_csv": {"schemaTitle": "system.Dataset", "instanceSchema": "", "metadataPath":
          "/tmp/outputs/test_csv/data"}, "train_csv": {"schemaTitle": "system.Dataset",
          "instanceSchema": "", "metadataPath": "/tmp/outputs/train_csv/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      artifacts:
      - {name: download-data-output_csv, path: /tmp/inputs/input_csv/data}
    outputs:
      artifacts:
      - {name: split-data-test_csv, path: /tmp/outputs/test_csv/data}
      - {name: split-data-train_csv, path: /tmp/outputs/train_csv/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  - name: train-model
    container:
      args:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet                 --no-warn-script-location
        'tensorflow' 'pandas' 'appengine-python-standard' 'kfp==1.7.0' || PIP_DISABLE_PIP_VERSION_CHECK=1
        python3 -m pip install --quiet                 --no-warn-script-location 'tensorflow'
        'pandas' 'appengine-python-standard' 'kfp==1.7.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp -d)
        printf "%s" "$0" > "$program_path/ephemeral_component.py"
        python3 -m kfp.components.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"
      - |2+

        from kfp.v2.dsl import *
        from typing import *

        def train_model(
            input_train_x: Input[Dataset], input_train_y: Input[Dataset],
            output_model: Output[Model], output_history: Output[Artifact]
        ):
            import pandas as pd
            import tensorflow as tf
            import pickle

            from tensorflow.keras import Model
            from tensorflow.keras.layers import Dense, Input

            norm_train_X = pd.read_csv(input_train_x.path)
            with open(input_train_y.path, "rb") as file:
                train_Y = pickle.load(file)

            def model(train_X):
                input_layer = Input(shape=len(train_X.columns))
                dense1 = Dense(units=128, activation='relu')(input_layer)
                dense2 = Dense(units=128, activation='relu')(dense1)
                dense3 = Dense(units=64, activation='relu')(dense2)

                y1_output = Dense(units=1, name='y1_output')(dense2)
                y2_output = Dense(units=1, name='y2_output')(dense3)

                model = Model(inputs=input_layer, outputs=[y1_output, y2_output])

                print(model.summary())

                return model

            model = model(norm_train_X)

            optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)
            model.compile(
                optimizer=optimizer,
                loss={'y1_output': 'mse', 'y2_output': 'mse'},
                metrics={
                    'y1_output': tf.keras.metrics.RootMeanSquaredError(),
                    'y2_output': tf.keras.metrics.RootMeanSquaredError()
                }
            )
            history = model.fit(norm_train_X, train_Y, epochs=100)
            model.save(output_model.path)

            with open(output_history.path, "wb") as file:
                train_Y = pickle.dump(history.history, file)

      - --executor_input
      - '{{$}}'
      - --function_to_execute
      - train_model
      - --input-train-x-output-path
      - '{{$.inputs.artifacts[''input_train_x''].path}}'
      - --input-train-y-output-path
      - '{{$.inputs.artifacts[''input_train_y''].path}}'
      - --output-model-output-path
      - '{{$.outputs.artifacts[''output_model''].path}}'
      - --output-history-output-path
      - '{{$.outputs.artifacts[''output_history''].path}}'
      command: [/kfp-launcher/launch, --mlmd_server_address, $(METADATA_GRPC_SERVICE_HOST),
        --mlmd_server_port, $(METADATA_GRPC_SERVICE_PORT), --runtime_info_json, $(KFP_V2_RUNTIME_INFO),
        --container_image, $(KFP_V2_IMAGE), --task_name, train-model, --pipeline_name,
        '{{inputs.parameters.pipeline-name}}', --run_id, $(KFP_RUN_ID), --run_resource,
        workflows.argoproj.io/$(WORKFLOW_ID), --namespace, $(KFP_NAMESPACE), --pod_name,
        $(KFP_POD_NAME), --pod_uid, $(KFP_POD_UID), --pipeline_root, '{{inputs.parameters.pipeline-output-directory}}',
        --enable_caching, $(ENABLE_CACHING), --, --]
      env:
      - name: KFP_POD_NAME
        valueFrom:
          fieldRef: {fieldPath: metadata.name}
      - name: KFP_POD_UID
        valueFrom:
          fieldRef: {fieldPath: metadata.uid}
      - name: KFP_NAMESPACE
        valueFrom:
          fieldRef: {fieldPath: metadata.namespace}
      - name: WORKFLOW_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''workflows.argoproj.io/workflow'']'}
      - name: KFP_RUN_ID
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipeline/runid'']'}
      - name: ENABLE_CACHING
        valueFrom:
          fieldRef: {fieldPath: 'metadata.labels[''pipelines.kubeflow.org/enable_caching'']'}
      - {name: KFP_V2_IMAGE, value: 'python:3.7'}
      - {name: KFP_V2_RUNTIME_INFO, value: '{"inputParameters": {}, "inputArtifacts":
          {"input_train_x": {"metadataPath": "/tmp/inputs/input_train_x/data", "schemaTitle":
          "system.Dataset", "instanceSchema": ""}, "input_train_y": {"metadataPath":
          "/tmp/inputs/input_train_y/data", "schemaTitle": "system.Dataset", "instanceSchema":
          ""}}, "outputParameters": {}, "outputArtifacts": {"output_history": {"schemaTitle":
          "system.Artifact", "instanceSchema": "", "metadataPath": "/tmp/outputs/output_history/data"},
          "output_model": {"schemaTitle": "system.Model", "instanceSchema": "", "metadataPath":
          "/tmp/outputs/output_model/data"}}}'}
      envFrom:
      - configMapRef: {name: metadata-grpc-configmap, optional: true}
      image: python:3.7
      volumeMounts:
      - {mountPath: /kfp-launcher, name: kfp-launcher}
    inputs:
      parameters:
      - {name: pipeline-name}
      - {name: pipeline-output-directory}
      artifacts:
      - {name: preprocess-data-output_train_x, path: /tmp/inputs/input_train_x/data}
      - {name: preprocess-data-output_train_y, path: /tmp/inputs/input_train_y/data}
    outputs:
      artifacts:
      - {name: train-model-output_history, path: /tmp/outputs/output_history/data}
      - {name: train-model-output_model, path: /tmp/outputs/output_model/data}
    metadata:
      annotations:
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/component_ref: '{}'
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.7.0
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/v2_component: "true"
        pipelines.kubeflow.org/enable_caching: "true"
    initContainers:
    - command: [launcher, --copy, /kfp-launcher/launch]
      image: gcr.io/ml-pipeline/kfp-launcher:1.7.0
      name: kfp-launcher
      mirrorVolumeMounts: true
    volumes:
    - {name: kfp-launcher}
  arguments:
    parameters:
    - {name: url}
    - {name: pipeline-output-directory, value: ''}
    - {name: pipeline-name, value: pipeline/ml-pipeline}
  serviceAccountName: pipeline-runner
