{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a96ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kfp\n",
    "\n",
    "from kfp.v2 import dsl\n",
    "\n",
    "from kfp.v2.dsl import component\n",
    "\n",
    "from kfp.v2.dsl import (\n",
    "    Input,\n",
    "    Output,\n",
    "    Artifact,\n",
    "    Dataset,\n",
    "    Model,\n",
    "    Metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2df49669",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('pipeline', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "111a5575",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"openpyxl\", \"appengine-python-standard\"],\n",
    "    output_component_file=\"pipeline/download_data_component.yaml\"\n",
    ")\n",
    "def download_data(url:str, output_csv:Output[Dataset]) -> None:\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.read_excel(url)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df.to_csv(output_csv.path, index=False)\n",
    "#compiler.Compiler().compile(download_data, package_path='pipeline/download_data_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8313b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"scikit-learn\", \"appengine-python-standard\"],\n",
    "    output_component_file=\"pipeline/split_data_component.yaml\"\n",
    ")\n",
    "def split_data(input_csv: Input[Dataset], train_csv: Output[Dataset], test_csv: Output[Dataset]):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    df = pd.read_csv(input_csv.path)\n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    train.to_csv(train_csv.path, index=False)\n",
    "    test.to_csv(test_csv.path, index=False)\n",
    "#compiler.Compiler().compile(download_data, package_path='pipeline/split_data_component.yaml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c44d08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ccab636",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"pandas\", \"numpy\", \"appengine-python-standard\"],\n",
    "    output_component_file=\"pipeline/preprocess_data_component.yaml\"\n",
    ")\n",
    "def preprocess_data(\n",
    "    input_train_csv: Input[Dataset], input_test_csv: Input[Dataset],\n",
    "    output_train_x: Output[Dataset], output_test_x: Output[Dataset],\n",
    "    output_train_y: Output[Artifact], output_test_y: Output[Artifact]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import pickle\n",
    "    \n",
    "    def format_output(data):\n",
    "        y1 = data.pop('Y1')\n",
    "        y1 = np.array(y1)\n",
    "        y2 = data.pop('Y2')\n",
    "        y2 = np.array(y2)\n",
    "        return y1, y2\n",
    "    \n",
    "    def norm(x, train_stats):\n",
    "        return (x - train_stats['mean']) / train_stats['std']\n",
    "    \n",
    "    train = pd.read_csv(input_train_csv.path)\n",
    "    test = pd.read_csv(input_test_csv.path)\n",
    "    \n",
    "    train_stats = train.describe()\n",
    "    train_stats.pop('Y1')\n",
    "    train_stats.pop('Y2')\n",
    "    train_stats = train_stats.transpose()\n",
    "    \n",
    "    train_Y = format_output(train)\n",
    "    with open(output_train_y.path, \"wb\") as file:\n",
    "        pickle.dump(train_Y, file)\n",
    "        \n",
    "    test_Y = format_output(test)\n",
    "    with open(output_test_y.path, \"wb\") as file:\n",
    "        pickle.dump(test_Y, file)\n",
    "    \n",
    "    norm_train_x = norm(train, train_stats)\n",
    "    norm_test_x = norm(test, train_stats)\n",
    "    \n",
    "    norm_train_x.to_csv(output_train_x.path, index=False)\n",
    "    norm_test_x.to_csv(output_test_x.path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "39bcaf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"pandas\", \"appengine-python-standard\"],\n",
    "    output_component_file=\"pipeline/train_model_component.yaml\"\n",
    ")\n",
    "def train_model(\n",
    "    input_train_x: Input[Dataset], input_train_y: Input[Dataset],\n",
    "    output_model: Output[Model], output_history: Output[Artifact]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    \n",
    "    from tensorflow.keras import Model\n",
    "    from tensorflow.keras.layers import Dense, Input\n",
    "    \n",
    "    norm_train_X = pd.read_csv(input_train_x.path)\n",
    "    with open(input_train_y.path, \"rb\") as file:\n",
    "        train_Y = pickle.load(file)\n",
    "        \n",
    "    def model(train_X):\n",
    "        input_layer = Input(shape=len(train_X.columns))\n",
    "        dense1 = Dense(units=128, activation='relu')(input_layer)\n",
    "        dense2 = Dense(units=128, activation='relu')(dense1)\n",
    "        dense3 = Dense(units=64, activation='relu')(dense2)\n",
    "        \n",
    "        y1_output = Dense(units=1, name='y1_output')(dense2)\n",
    "        y2_output = Dense(units=1, name='y2_output')(dense3)\n",
    "        \n",
    "        model = Model(inputs=input_layer, outputs=[y1_output, y2_output])\n",
    "        \n",
    "        print(model.summary())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    model = model(norm_train_X)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss={'y1_output': 'mse', 'y2_output': 'mse'},\n",
    "        metrics={\n",
    "            'y1_output': tf.keras.metrics.RootMeanSquaredError(),\n",
    "            'y2_output': tf.keras.metrics.RootMeanSquaredError()\n",
    "        }\n",
    "    )\n",
    "    history = model.fit(norm_train_X, train_Y, epochs=100)\n",
    "    model.save(output_model.path)\n",
    "    \n",
    "    with open(output_history.path, \"wb\") as file:\n",
    "        train_Y = pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "217a094d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@component(\n",
    "    packages_to_install=[\"tensorflow\", \"pandas\", \"appengine-python-standard\"],\n",
    "    output_component_file=\"pipeline/eval_model_component.yaml\"\n",
    ")\n",
    "def eval_model(\n",
    "    input_model: Input[Model], input_history: Input[Artifact],\n",
    "    input_test_x: Input[Dataset], input_test_y: Input[Artifact],\n",
    "    MLPipeline_Metrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import tensorflow as tf\n",
    "    import pickle\n",
    "    \n",
    "    model = tf.keras.models.load_model(input_model.path)\n",
    "    norm_test_X = pd.read_csv(input_test_x.path)\n",
    "    with open(input_test_y.path, \"rb\") as file:\n",
    "        test_Y = pickle.load(file)\n",
    "    \n",
    "    loss, Y1_loss, Y2_loss, Y1_rmse, Y2_rmse = model.evaluate(x=norm_test_X, y=test_Y)\n",
    "    print(\"Loss = {}, Y1_loss = {}, Y1_mse = {}, Y2_loss = {}, Y2_mse = {}\".format(loss, Y1_loss, Y1_rmse, Y2_loss, Y2_rmse))\n",
    "    \n",
    "    MLPipeline_Metrics.log_metric(\"loss\", loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y1_loss\", Y2_loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y2_loss\", Y2_loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y1_rmse\", Y2_loss)\n",
    "    MLPipeline_Metrics.log_metric(\"Y2_rmse\", Y2_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7601f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(\n",
    "    name=\"ml-pipeline\"\n",
    ")\n",
    "def my_pipeline(url: str):\n",
    "    download_data_task = download_data(url)\n",
    "    split_data_task = split_data(input_csv=download_data_task.outputs['output_csv'])\n",
    "    preprocess_data_task = preprocess_data(\n",
    "        input_train_csv=split_data_task.outputs['train_csv'],\n",
    "        input_test_csv=split_data_task.outputs['test_csv']\n",
    "    )\n",
    "    train_model_task = train_model(input_train_x=preprocess_data_task.outputs['output_train_x'],\n",
    "                                  input_train_y=preprocess_data_task.outputs['output_train_y']\n",
    "                                  )\n",
    "    \n",
    "    eval_model_task = eval_model(input_model=train_model_task.outputs['output_model'],\n",
    "                                 input_history=train_model_task.outputs['output_history'],\n",
    "                                 input_test_x=preprocess_data_task.outputs['output_test_x'],\n",
    "                                 input_test_y=preprocess_data_task.outputs['output_test_y']\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5daada4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfp.compiler.Compiler(mode=kfp.dsl.PipelineExecutionMode.V2_COMPATIBLE).compile(pipeline_func=my_pipeline, package_path='pipeline/pipeline.yaml')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
